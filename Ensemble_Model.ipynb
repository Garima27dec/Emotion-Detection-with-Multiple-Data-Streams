{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuqzirOeAHblDcyXaBXgQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Garima27dec/Emotion-Detection-with-Multiple-Data-Streams/blob/main/Ensemble_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing and installing Libraries**"
      ],
      "metadata": {
        "id": "D2cdBxACchPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-text==2.12.0\n",
        "!pip3 install opencv-python\n",
        "!pip install pydub\n",
        "!pip install SpeechRecognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzN6kSoNo4Ec",
        "outputId": "71a6e347-3e72-4c1c-ebf0-84ddfd0bcc6c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-text==2.12.0\n",
            "  Downloading tensorflow_text-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.12.0) (0.14.0)\n",
            "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.41.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.12.0\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2023.7.22)\n",
            "Installing collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM,Dropout,TimeDistributed, Flatten, Bidirectional, Input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import zscore\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential, Model\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.metrics import CategoricalAccuracy\n",
        "import re\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import glob\n",
        "import statistics\n",
        "from scipy.ndimage import zoom\n",
        "from os import path\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import IPython\n",
        "from IPython.display import Audio, Image\n",
        "import speech_recognition"
      ],
      "metadata": {
        "id": "lnOsqziwahJS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Connecting to Storage - to import saved models**"
      ],
      "metadata": {
        "id": "H5Mae4ZFcxsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPICYodioei4",
        "outputId": "691682ee-c8f6-4f93-85b5-3e3bf84d047f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Loading Face, Speech, Text Emotion Recognition Models**"
      ],
      "metadata": {
        "id": "NQ6oAaRSdQ_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "with open('/content/drive/MyDrive/Emotion_Recognition/FER/savedmodels/model_3.json','r') as f:\n",
        "    json = f.read()\n",
        "model_fer = model_from_json(json)\n",
        "\n",
        "model_fer.load_weights('/content/drive/MyDrive/Emotion_Recognition/FER/savedmodels/model_3.h5')\n",
        "print(\"Loaded FER model from disk\")"
      ],
      "metadata": {
        "id": "-JuP8u6gool0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16da7229-03d1-464b-f4b4-d892c01fbcd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded FER model from disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model_ser = load_model('/content/drive/MyDrive/Emotion_Recognition/SER/savedmodel/[CNN-LSTM]M.h5')\n",
        "\n",
        "model_ser.load_weights('/content/drive/MyDrive/Emotion_Recognition/SER/savedmodel/[CNN-LSTM]W.h5')\n",
        "print(\"Loaded SER model from disk\")"
      ],
      "metadata": {
        "id": "xNJn0U-ttrCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec807082-6624-4cbb-8f06-df7b9f9189ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SER model from disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model_ter = tf.keras.models.load_model(('/content/drive/MyDrive/Emotion_Recognition/Text_ER/BERT_M.h5'), custom_objects={'KerasLayer':hub.KerasLayer})\n",
        "model_ter.load_weights('/content/drive/MyDrive/Emotion_Recognition/Text_ER/BERT_W.h5')\n",
        "print(\"Loaded Text_ER model from disk\")"
      ],
      "metadata": {
        "id": "NOVwkVI2x5P2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "265c096d-b1df-4c54-edf9-7fa3aec997c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Text_ER model from disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Defining functions for Data Pre-processing**"
      ],
      "metadata": {
        "id": "8zf22RWhlmrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape_x = 48\n",
        "shape_y = 48"
      ],
      "metadata": {
        "id": "6VWd0-3_l77J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_face(frame):\n",
        "\n",
        "    #Cascade classifier pre-trained model\n",
        "    cascPath = '/content/drive/MyDrive/Emotion_Recognition/FER/haarcascade_frontalface_default.xml'\n",
        "    faceCascade = cv2.CascadeClassifier(cascPath)\n",
        "\n",
        "    #BGR -> Gray conversion\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    #Cascade MultiScale classifier\n",
        "    detected_faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=6,\n",
        "                                                  minSize=(shape_x, shape_y),\n",
        "                                                  flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "    coord = []\n",
        "\n",
        "    for x, y, w, h in detected_faces :\n",
        "        if w > 100 :\n",
        "            sub_img=frame[y:y+h,x:x+w]\n",
        "            #cv2.rectangle(frame,(x,y),(x+w,y+h),(0, 255,255),1)\n",
        "            coord.append([x,y,w,h])\n",
        "\n",
        "    return gray, detected_faces, coord"
      ],
      "metadata": {
        "id": "QhsxcvzgllgK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_face_features(faces, offset_coefficients=(0.075, 0.05)):\n",
        "    gray = faces[0]\n",
        "    detected_face = faces[1]\n",
        "\n",
        "    new_face = []\n",
        "\n",
        "    for det in detected_face :\n",
        "\n",
        "        x, y, w, h = det\n",
        "\n",
        "\n",
        "        #Offset coefficient, np.floor takes the lowest integer (delete border of the image)\n",
        "        horizontal_offset = np.int(np.floor(offset_coefficients[0] * w))\n",
        "        vertical_offset = np.int(np.floor(offset_coefficients[1] * h))\n",
        "\n",
        "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
        "\n",
        "\n",
        "        new_extracted_face = zoom(extracted_face, (shape_x / extracted_face.shape[0],shape_y / extracted_face.shape[1]))\n",
        "        #cast type float\n",
        "        new_extracted_face = new_extracted_face.astype(np.float32)\n",
        "        #scale\n",
        "        new_extracted_face /= float(new_extracted_face.max())\n",
        "        #print(new_extracted_face)\n",
        "\n",
        "        new_face.append(new_extracted_face)\n",
        "\n",
        "    return new_face"
      ],
      "metadata": {
        "id": "dmYkF7ZUmDWw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getFrame(sec):\n",
        "    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
        "    hasFrames,image = vidcap.read()\n",
        "    if hasFrames:\n",
        "        cv2.imwrite(\"/content/drive/MyDrive/Emotion_Recognition/FER/Results/\"+\"image\"+str(count)+\".jpg\", image)     # save frame as JPG file\n",
        "    return hasFrames\n"
      ],
      "metadata": {
        "id": "AY7Q-EKX9RxL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_fer(argument):\n",
        "    labels = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad' , 5:'Surprise', 6:'Neutral'}\n",
        "    return(labels.get(argument, \"Invalid emotion\"))"
      ],
      "metadata": {
        "id": "wGGtF8-VmuaR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_ser(argument):\n",
        "    labels = {0:'Neutral', 1:'Happy', 2:'Sad', 3:'Angry', 4:'Fear' , 5:'Disgust', 6:'Surprise'}\n",
        "    return(labels.get(argument, \"Invalid emotion\"))"
      ],
      "metadata": {
        "id": "sehyyeLl4DDN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mel_spectrogram(y, sr=16000, n_fft=512, win_length=256, hop_length=128, window='hamming', n_mels=128, fmax=4000):\n",
        "\n",
        "    # Compute spectogram\n",
        "    mel_spect = np.abs(librosa.stft(y, n_fft=n_fft, window=window, win_length=win_length, hop_length=hop_length)) ** 2\n",
        "\n",
        "    # Compute mel spectrogram\n",
        "    mel_spect = librosa.feature.melspectrogram(S=mel_spect, sr=sr, n_mels=n_mels, fmax=fmax)\n",
        "\n",
        "    # Compute log-mel spectrogram\n",
        "    mel_spect = librosa.power_to_db(mel_spect, ref=np.max)\n",
        "\n",
        "    return mel_spect"
      ],
      "metadata": {
        "id": "sWgdVM7H28uJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time distributed parameters\n",
        "win_ts = 128\n",
        "hop_ts = 64\n",
        "\n",
        "# Split spectrogram into frames\n",
        "def frame(x, win_step=128, win_size=64):\n",
        "    nb_frames = 1 + int((x.shape[2] - win_size) / win_step)\n",
        "    frames = np.zeros((x.shape[0], nb_frames, x.shape[1], win_size)).astype(np.float32)\n",
        "    for t in range(nb_frames):\n",
        "        frames[:,t,:,:] = np.copy(x[:,:,(t * win_step):(t * win_step + win_size)]).astype(np.float32)\n",
        "    return frames"
      ],
      "metadata": {
        "id": "ctuC6C3P5ob6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_prediction(test_string):\n",
        "  class_prob = model_ter.predict(test_string, batch_size=1)[0]\n",
        "  if np.argmax(class_prob)==0:\n",
        "    return(\"Angry\")\n",
        "  elif np.argmax(class_prob)==1:\n",
        "    return(\"Disgust\")\n",
        "  elif np.argmax(class_prob)==2:\n",
        "    return(\"Fear\")\n",
        "  elif np.argmax(class_prob)==3:\n",
        "    return(\"Happy\")\n",
        "  elif np.argmax(class_prob)==4:\n",
        "    return(\"Neutral\")\n",
        "  elif np.argmax(class_prob)==5:\n",
        "    return(\"Sad\")\n",
        "  else:\n",
        "    return(\"Surprise\")"
      ],
      "metadata": {
        "id": "EH622AwEDbEa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ensemble Model**"
      ],
      "metadata": {
        "id": "Jr-67tXhpiso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src = \"/content/drive/MyDrive/Emotion_Recognition/Model_Testing/MELD.Raw/train_splits/dia0_utt0.mp4\""
      ],
      "metadata": {
        "id": "1t_iR4Opl-1N"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_emotion = []\n",
        "final_probability =[]\n",
        "\n",
        "src = \"/content/drive/MyDrive/Emotion_Recognition/Model_Testing/MELD.Raw/train_splits/dia0_utt0.mp4\"\n",
        "\n",
        "vidcap = cv2.VideoCapture(src)\n",
        "\n",
        "sec = 0\n",
        "frameRate = 0.5 #//it will capture image in each 0.5 second\n",
        "count=1\n",
        "success = getFrame(sec)\n",
        "while success:\n",
        "    count = count + 1\n",
        "    sec = sec + frameRate\n",
        "    sec = round(sec, 2)\n",
        "    success = getFrame(sec)\n",
        "\n",
        "emotions = []\n",
        "emotions_prob = []\n",
        "\n",
        "source = glob.glob('/content/drive/MyDrive/Emotion_Recognition/FER/Results/*')\n",
        "\n",
        "for i in source:\n",
        "  face = cv2.imread(i)\n",
        "  for face in extract_face_features(detect_face(face)) :\n",
        "    to_predict = np.reshape(face.flatten(), (1,48,48,1))\n",
        "    res1 = model_fer.predict(to_predict)\n",
        "    result_num = np.argmax(res1)\n",
        "    prob = np.amax(res1)\n",
        "    emotion_per_frame = get_label_fer(result_num)\n",
        "    #print (emotion_per_frame)\n",
        "    emotions.append(emotion_per_frame)\n",
        "    emotions_prob.append(prob)\n",
        "\n",
        "#print(emotions)\n",
        "#print(emotions_prob)\n",
        "print(\"Emotion from FER: \", statistics.mode(emotions))\n",
        "print(\"Emotion Probability: \", statistics.mean(emotions_prob))\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Emotion_Recognition/FER/Results/'\n",
        "\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.jpg'):\n",
        "            os.remove(os.path.join(root, file))\n",
        "\n",
        "dst = \"/content/drive/MyDrive/Emotion_Recognition/Model_Testing/wav_files/test0.wav\"\n",
        "\n",
        "# convert mp4 to wav\n",
        "sound = AudioSegment.from_file(src,format=\"mp4\")\n",
        "sound.export(dst, format=\"wav\")\n",
        "\n",
        "# Sample rate (16.0 kHz)\n",
        "sample_rate = 16000\n",
        "\n",
        "# Max pad lenght (3.0 sec)\n",
        "max_pad_len = 49100\n",
        "\n",
        "        # Read audio file\n",
        "y, sr = librosa.core.load(dst, sr=sample_rate, offset=0.5)\n",
        "\n",
        "        # Z-normalization\n",
        "y = zscore(y)\n",
        "\n",
        "        # Padding or truncated signal\n",
        "if len(y) < max_pad_len:\n",
        "  y_padded = np.zeros(max_pad_len)\n",
        "  y_padded[:len(y)] = y\n",
        "  y = y_padded\n",
        "elif len(y) > max_pad_len:\n",
        "  y = np.asarray(y[:max_pad_len])\n",
        "\n",
        "# Data Preprocessing\n",
        "mel_spect = np.asarray(mel_spectrogram(y))\n",
        "mel_spect = np.reshape(mel_spect,(1,128,384))\n",
        "input = frame(mel_spect, hop_ts, win_ts)\n",
        "input = input.reshape(input.shape[0], input.shape[1] , input.shape[2], input.shape[3], 1)\n",
        "\n",
        "for speech in input:\n",
        "    res = model_ser.predict(input)\n",
        "    result_num2 = np.argmax(res)\n",
        "    #print(result_num)\n",
        "    print(\"Emotion from SER:\", get_label_ser(result_num2))\n",
        "    print(\"Emotion Probability: \", np.amax(res))\n",
        "\n",
        "r = speech_recognition.Recognizer()\n",
        "\n",
        "hellow=speech_recognition.AudioFile(dst)\n",
        "with hellow as source:\n",
        "    audio = r.record(source)\n",
        "try:\n",
        "    s = r.recognize_google(audio)\n",
        "    print(\"Text: \"+s)\n",
        "except Exception as e:\n",
        "    print(\"Exception: \"+str(e))\n",
        "\n",
        "print(\"Emotion from Text: \", text_prediction([s]))\n",
        "print(\"Emotion Probability: \", np.amax(model_ter.predict([s])))\n",
        "\n",
        "compile_emotion = [statistics.mode(emotions), get_label_ser(result_num2), text_prediction([s])]\n",
        "compile_emotion_prob = [statistics.mean(emotions_prob), np.amax(res), np.amax(model_ter.predict([s]))]\n",
        "\n",
        "if ((compile_emotion[0] != compile_emotion[1]) and (compile_emotion[0] != compile_emotion[2]) and (compile_emotion[1] != compile_emotion[2])):\n",
        "  idx = compile_emotion_prob.index(max(compile_emotion_prob))\n",
        "  final_emotion.append(compile_emotion[idx])\n",
        "  final_probability.append(max(compile_emotion_prob))\n",
        "\n",
        "else:\n",
        "  final_emotion.append(statistics.mode(compile_emotion))\n",
        "\n",
        "if ((compile_emotion[0] == compile_emotion[1]) and (compile_emotion[0] == compile_emotion[2])):\n",
        "  final_probability.append(statistics.mean(compile_emotion_prob))\n",
        "elif ((compile_emotion[0] == compile_emotion[1]) and (compile_emotion[0] != compile_emotion[2])) :\n",
        "  final_probability.append((compile_emotion_prob[0]+compile_emotion_prob[1])/2)\n",
        "elif ((compile_emotion[0] == compile_emotion[2]) and (compile_emotion[0] != compile_emotion[1])):\n",
        "  final_probability.append((compile_emotion_prob[0]+compile_emotion_prob[2])/2)\n",
        "else:\n",
        "  final_probability.append((compile_emotion_prob[1]+compile_emotion_prob[2])/2)\n",
        "\n",
        "print(\"Final Emotion: \", final_emotion)\n",
        "print(\"Final Emotion Probability: \", final_probability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glxPkokbb8aE",
        "outputId": "3a6a010f-6018-4087-f5b0-fcc16f98595f"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 50ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-62d6636c3fdd>:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  horizontal_offset = np.int(np.floor(offset_coefficients[0] * w))\n",
            "<ipython-input-11-62d6636c3fdd>:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  vertical_offset = np.int(np.floor(offset_coefficients[1] * h))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "Emotion from FER:  Angry\n",
            "Emotion Probability:  0.8369248\n",
            "1/1 [==============================] - 0s 228ms/step\n",
            "Emotion from SER: Neutral\n",
            "Emotion Probability:  0.8652872\n",
            "Text: transition from the kl5 to gr6 system\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Emotion from Text:  Neutral\n",
            "1/1 [==============================] - 1s 976ms/step\n",
            "Emotion Probability:  0.43641794\n",
            "1/1 [==============================] - 1s 946ms/step\n",
            "1/1 [==============================] - 1s 840ms/step\n",
            "Final Emotion:  ['Neutral']\n",
            "Final Emotion Probability:  [0.6508525609970093]\n"
          ]
        }
      ]
    }
  ]
}